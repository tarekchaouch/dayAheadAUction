{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-30T15:06:35.485959Z",
     "start_time": "2024-05-30T15:06:35.480464Z"
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T12:37:46.834087Z",
     "start_time": "2024-05-30T12:37:43.432763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_frames = []\n",
    "for file in os.listdir('/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves'):\n",
    "    data_frames.append(pd.read_csv(os.path.join('/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves', file), header=1))\n"
   ],
   "id": "9662bbc319145ef1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T12:37:54.281880Z",
     "start_time": "2024-05-30T12:37:54.205378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_ac = \"/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves\"\n",
    "a1 =pd.read_csv(os.path.join(path_ac, \"auction_aggregated_curves_germany_luxembourg_20220506.csv\"),header=1)\n",
    "a2 = pd.read_csv(os.path.join(path_ac, \"auction_aggregated_curves_germany_luxembourg_20220101.csv\"),header=1)\n",
    "a3= pd.read_csv(os.path.join(path_ac, \"auction_aggregated_curves_germany_luxembourg_20220221.csv\"),header=1)"
   ],
   "id": "18598dfe97fa8382",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "a1.info()",
   "id": "4a248f094c306510",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(a1)\n",
    "df_first_h = df[(df['Hour'] == 1)]\n",
    "c =df_first_h[df_first_h['Sale/Purchase'] == 'Sell'].count()['Date']\n",
    "print(\"The numbers of sells in the first hour:\", c)\n",
    "c =df_first_h[df_first_h['Sale/Purchase'] == 'Purchase'].count()['Date']\n",
    "print(\"The numbers of purchases in the first hour:\", c)"
   ],
   "id": "1f0ed913d7f662ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "hourly_counts = df.groupby(['Hour', 'Sale/Purchase']).size().unstack(fill_value=0)\n",
    "\n",
    "# Get unique hours\n",
    "hours = np.sort(df['Hour'].unique())\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "\n",
    "# Set the positions of the bars on the x-axis\n",
    "r1 = np.arange(len(hours))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Plotting\n",
    "plt.bar(r1, hourly_counts['Purchase'], color='orange', width=bar_width, label='Purchase')\n",
    "plt.bar(r2, hourly_counts['Sell'], color='blue', width=bar_width, label='Sell')\n",
    "\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Number of Sales and Purchases by Hour')\n",
    "plt.xticks([r + bar_width/2 for r in range(len(hours))], hours)\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "a971a59b01521419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-30T09:28:18.912903Z",
     "start_time": "2024-05-30T09:28:18.364252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for df in data_frames:\n",
    "   df['Date']= pd.to_datetime(df['Date'],format='%d/%m/%Y')\n",
    "data_frames_sorted = sorted(data_frames, key=lambda x: x['Date'].min())\n"
   ],
   "id": "a9e9991854c4de64",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plotting for each DataFrame\n",
    "for i, df in enumerate(data_frames_sorted, start=1):\n",
    "    # Grouping by 'Hour' and 'Sale/Purchase' and getting counts\n",
    "    hourly_counts = df.groupby(['Hour', 'Sale/Purchase']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Get unique hours\n",
    "    hours = np.sort(df['Hour'].unique())\n",
    "\n",
    "    # Set the width of the bars\n",
    "    bar_width = 0.35\n",
    "\n",
    "    # Set the positions of the bars on the x-axis\n",
    "    r1 = np.arange(len(hours))\n",
    "    r2 = [x + bar_width for x in r1]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.bar(r1, hourly_counts['Purchase'], color='orange', width=bar_width, label='Purchase')\n",
    "    plt.bar(r2, hourly_counts['Sell'], color='blue', width=bar_width, label='Sell')\n",
    "\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(f'Number of Sales and Purchases by Hour - for {str(df[\"Date\"][0])}')\n",
    "    plt.xticks([r + bar_width/2 for r in range(len(hours))], hours)\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "2cd500fdbc06809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#plot the price of each index\n",
    "for df in data_frames_sorted:\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(df.index,df[\"Price\"])\n",
    "\n",
    "plt.show()"
   ],
   "id": "e07eb1b025c29540",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head(20)",
   "id": "726f95c7d5efda28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The relationship between the prices and volumes\n",
    "\n",
    "# Calculate the correlation between Price and Volume\n",
    "for df in data_frames_sorted:\n",
    "    correlation = df['Volume'].corr(df['Price'])\n",
    "    print(f\"Correlation between Price and Volume: {correlation}\")\n",
    "    # Scatter plot of Price vs Volume\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(df['Volume'], df['Price'], color='blue')\n",
    "    plt.title('Price vs Volume')\n",
    "    plt.xlabel('Volume')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #Observation: Negative correlation which means when the Volume variable increases, the Price variable decreases. They move in opposite direction\n"
   ],
   "id": "c06b56f134573d4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for df in data_frames_sorted:\n",
    "\n",
    "    # Separate the data into 'Sell' and 'Purchase'\n",
    "    sell_data = df[df['Sale/Purchase'] == 'Sell']\n",
    "    purchase_data = df[df['Sale/Purchase'] == 'Purchase']\n",
    "    \n",
    "    # Scatter plot for Sell\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(sell_data['Volume'], sell_data['Price'], color='red', label='Sell')\n",
    "    plt.scatter(purchase_data['Volume'], purchase_data['Price'], color='green', label='Purchase')\n",
    "    plt.title('Price vs Volume (Sale vs Purchase)')\n",
    "    plt.ylabel('Price')\n",
    "    plt.xlabel('Volume')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #Observation:\n",
    "    #Ascending curves for Sells / Descending curves for Purchases\n",
    "    # volume is under 10k : when the Price is negative we have only Sales, when the price is positive we have only purchases.\n",
    "    # Volume between 10k and 20k: when the price is negative we have only Sales, when the price is positive only Purchase. \n",
    "    # Volume between 20k and 30k : the price is positive and we have MCP.\n",
    "    # Volume is over 30k: When the price is negative Purchase and when the price is positive only sell.\n",
    "    # At 25k the values of the prices mirror each other   "
   ],
   "id": "ab0fcb980f08ed5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#How many pairs do I have (pair of price and volume) with high prices and how many pairs I have with low prices\n",
    "for df in data_frames_sorted:\n",
    "    # Define a threshold for high and low prices\n",
    "    threshold = 0  # Adjustable\n",
    "    \n",
    "    # Categorize data into high and low prices\n",
    "    df['Price_Category'] = ['High' if price > threshold else 'Low' for price in df['Price']]\n",
    "    \n",
    "    # Count the pairs of high and low prices\n",
    "    high_price_count = df[df['Price_Category'] == 'High'].shape[0]\n",
    "    low_price_count = df[df['Price_Category'] == 'Low'].shape[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df[df['Price_Category'] == 'High']['Volume'], bins=10, alpha=0.5, label='High Price', color='red')\n",
    "    plt.hist(df[df['Price_Category'] == 'Low']['Volume'], bins=10, alpha=0.5, label='Low Price', color='green')\n",
    "    plt.title(f'High Prices({high_price_count}) and Low Prices({low_price_count})')\n",
    "    plt.xlabel('Volume')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    #Observation : High prices occurs at most when the volume is between 20k and 30k "
   ],
   "id": "ca3f7813bec905a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#-Distribution per Hour: is there a concrete Hour in which we see many sell bids\n",
    "\n",
    "for df in data_frames_sorted:\n",
    "    def find_peak_sell_hour_with_plot(df):\n",
    "        # Filter the DataFrame to include only sell bids\n",
    "        sell_bids = df[df['Sale/Purchase'] == 'Sell']\n",
    "    \n",
    "        # Count the number of sell bids for each hour\n",
    "        sell_bids_by_hour = sell_bids['Hour'].value_counts().sort_index()\n",
    "    \n",
    "        # Find the hour with the maximum sell bids\n",
    "        peak_sell_hour = sell_bids_by_hour.idxmax()\n",
    "        max_sell_bids = sell_bids_by_hour.max()\n",
    "    \n",
    "        # Plotting the results\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sell_bids_by_hour.plot(kind='bar', color='skyblue')\n",
    "        plt.axhline(y=max_sell_bids, color='r', linestyle='--', label=f'Peak Hour: {peak_sell_hour}:00')\n",
    "        plt.title('Number of Sell Bids by Hour')\n",
    "        plt.xlabel('Hour of the Day')\n",
    "        plt.ylabel('Number of Sell Bids')\n",
    "        plt.legend()\n",
    "        plt.grid(axis='y')\n",
    "        plt.show()\n",
    "    \n",
    "        return peak_sell_hour, max_sell_bids\n",
    "    \n",
    "    # Use the function and plot the data\n",
    "    peak_hour, count = find_peak_sell_hour_with_plot(df)\n",
    "    print(f\"The hour with the most sell bids is {peak_hour}:00 with {count} sell bids.\")\n",
    "    \n",
    "    #Observation: Peak hours occurs often at Hours [1,3,7,10,24]"
   ],
   "id": "4a44c55357c2f2ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#Count: number of rows that are for a sell side for a specific Hour\n",
    "for df in data_frames_sorted:\n",
    "    # Initialize lists to store hourly data\n",
    "    hourly_volumes = []\n",
    "\n",
    "    # Iterate over each hour from 1 to 24\n",
    "    for hour in range(1, 25):\n",
    "        # Filter the DataFrame to include only rows for the current hour\n",
    "        hour_data = df[df['Hour'] == hour]\n",
    "        hour_data = hour_data[hour_data['Sale/Purchase'] == 'Sell']\n",
    "        # Sum the volumes for the current hour\n",
    "        total_volume_hour = hour_data['Volume'].count()\n",
    "        hourly_volumes.append(total_volume_hour)\n",
    "\n",
    "    # Plotting the result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, 25), hourly_volumes, color='blue')\n",
    "    plt.title('Number of Sale bids for Each Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "\n",
    "#Observation: Number of Sale bids is mostly at its maximum during the first and last hours"
   ],
   "id": "e4cefa01986c088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#Quantity: the sum of the Volumes of all rows according to each hour\n",
    "\n",
    "for df in data_frames_sorted:\n",
    "    # Initialize lists to store hourly data\n",
    "    hourly_volumes = []\n",
    "\n",
    "    # Iterate over each hour from 1 to 24\n",
    "    for hour in range(1, 25):\n",
    "        # Filter the DataFrame to include only rows for the current hour\n",
    "        hour_data = df[df['Hour'] == hour]\n",
    "\n",
    "        # Sum the volumes for the current hour\n",
    "        total_volume_hour = hour_data['Volume'].sum()\n",
    "        hourly_volumes.append(total_volume_hour)\n",
    "\n",
    "    # Plotting the result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, 25), hourly_volumes, color='blue')\n",
    "    plt.title('Total Volume for Each Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Total Volume')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    #observation : Total volume is mostly at its maximum between the 10th and 15th hour \n"
   ],
   "id": "b499bf9b94730702",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "#Count: number of rows that are for a Purchase side for a specific Hour\n",
    "for df in data_frames_sorted:\n",
    "    # Initialize lists to store hourly data\n",
    "    hourly_volumes = []\n",
    "\n",
    "    # Iterate over each hour from 1 to 24\n",
    "    for hour in range(1, 25):\n",
    "        # Filter the DataFrame to include only rows for the current hour\n",
    "        hour_data = df[df['Hour'] == hour]\n",
    "        hour_data = hour_data[hour_data['Sale/Purchase'] == 'Purchase']\n",
    "        # Sum the volumes for the current hour\n",
    "        total_volume_hour = hour_data['Volume'].count()\n",
    "        hourly_volumes.append(total_volume_hour)\n",
    "\n",
    "    # Plotting the result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, 25), hourly_volumes, color='blue')\n",
    "    plt.title('Number of Purchase bids for Each Hour')\n",
    "    plt.xlabel('Hour of the Day')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(axis='y')\n",
    "    plt.show()\n",
    "    \n",
    "    #Observation Number of Purchase bids is mostly at its maximum during the 12th hour and last 4 hours"
   ],
   "id": "83c3ce09162b3e0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220101.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "data.head()\n"
   ],
   "id": "d7bc5e3dd2f5eb73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read the file again with proper header handling\n",
    "data = pd.read_csv(file_path, header=1)\n",
    "\n",
    "# Rename the columns for easier handling\n",
    "data.columns = ['Date', 'Week', 'Week_Day', 'Hour', 'Price', 'Volume', 'Sale_Purchase']\n",
    "\n",
    "# Drop rows where the Price is NaN\n",
    "data = data.dropna(subset=['Price'])\n",
    "\n",
    "# Convert Price to numeric type\n",
    "data['Price'] = pd.to_numeric(data['Price'], errors='coerce')\n",
    "\n",
    "# Display the cleaned dataframe\n",
    "data.head()\n"
   ],
   "id": "2148b03248053f6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function to classify generators based on the Price column\n",
    "def classify_generator(price):\n",
    "    if price < 41:\n",
    "        return 'Low Cost'\n",
    "    elif 41 <= price <= 85:\n",
    "        return 'Mid Cost'\n",
    "    elif price >= 162:\n",
    "        return 'Expensive Cost'\n",
    "\n",
    "# Apply the classification function to the Price column\n",
    "data['Generator_Type'] = data['Price'].apply(classify_generator)\n",
    "\n",
    "# Display the dataframe with the new classification column\n",
    "data.head()\n"
   ],
   "id": "5f7e50e93d20d63e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Function to classify generators based on the Price column\n",
    "def classify_generator(price):\n",
    "    if price < 41:\n",
    "        return 'Low Cost /  Renewable Sources'\n",
    "    elif 41 <= price <= 85:\n",
    "        return 'Mid Cost / Gas'\n",
    "    elif price >= 162:\n",
    "        return 'Expensive Cost /  Oil'\n",
    "\n",
    "# Function to plot a histogram of the Generator_Type column\n",
    "def plot_generator_classification_histogram(data, filename):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    data['Generator_Type'].value_counts().plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of Generator Types in {filename}', fontsize=16)\n",
    "    plt.xlabel('Generator Type', fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves'\n",
    "\n",
    "# Process each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data = pd.read_csv(file_path, header=1)\n",
    "        data.columns = ['Date', 'Week', 'Week_Day', 'Hour', 'Price', 'Volume', 'Sale_Purchase']\n",
    "        data = data.dropna(subset=['Price'])\n",
    "        data['Price'] = pd.to_numeric(data['Price'], errors='coerce')\n",
    "        data['Generator_Type'] = data['Price'].apply(classify_generator)\n",
    "        plot_generator_classification_histogram(data, filename)\n"
   ],
   "id": "6bec660bbb7225f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#seasonal trends\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data files\n",
    "files = [\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220101.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220201.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220301.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220401.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220501.csv', '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220131.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220228.csv',\n",
    "    '/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220331.csv','/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/Aggregated curves/auction_aggregated_curves_germany_luxembourg_20220430.csv'    \n",
    "]\n",
    "\n",
    "data_frames = [pd.read_csv(file) for file in files]\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data\n",
    "data.head()\n"
   ],
   "id": "81b4f2cd4761304a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Function to read first few lines of each file to inspect structure\n",
    "def read_first_lines(file_path, num_lines=5):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [file.readline() for _ in range(num_lines)]\n",
    "    return lines\n",
    "\n",
    "# Read and display the first few lines of each file\n",
    "file_samples = [read_first_lines(file) for file in files]\n",
    "\n",
    "file_samples\n"
   ],
   "id": "c08a4288c3513601",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read and combine data from each file, skipping the first line\n",
    "data_frames = [pd.read_csv(file, skiprows=1) for file in files]\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the combined data to verify\n",
    "combined_data.head()\n"
   ],
   "id": "875722f40dfd9c27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the Date column to datetime format\n",
    "combined_data['Date'] = pd.to_datetime(combined_data['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Extract month and year for better grouping\n",
    "combined_data['Year'] = combined_data['Date'].dt.year\n",
    "combined_data['Month'] = combined_data['Date'].dt.month\n",
    "\n",
    "# Display the processed data\n",
    "combined_data.head()\n"
   ],
   "id": "47ea3fdc682384ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Group data by month and calculate average price and volume\n",
    "monthly_data = combined_data.groupby(['Year', 'Month']).agg({'Price': 'mean', 'Volume': 'mean'}).reset_index()\n",
    "\n",
    "# Plot the average price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_data['Month'], monthly_data['Price'], marker='o', linestyle='-')\n",
    "plt.title('Average Price Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the average volume over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(monthly_data['Month'], monthly_data['Volume'], marker='o', linestyle='-')\n",
    "plt.title('Average Volume Over Time')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Volume')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# The plots show the average price and volume over time for the first five months of 2022.\n",
    "# we can observe the following:\n",
    "# Average Price Over Time: There is a noticeable fluctuation in average prices across the months. Prices seem to vary\n",
    "# Average Volume Over Time: volume shows variability across the months.\n",
    "# Increase of the average price for winter season and decrease for spring season\n",
    "# Decrease of the average Volume for January and February and decrease for the last months"
   ],
   "id": "389b4fa4ab45222e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution curve of the prices\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# List the CSV files within the  folder\n",
    "curves_folder = os.path.join('/Users/tarekchaouch/Desktop/germany/Day-Ahead Auction/Hourly/Current/', 'Aggregated curves')\n",
    "csv_files = [os.path.join(curves_folder, file) for file in os.listdir(curves_folder) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold the price data from all files\n",
    "all_prices = []\n",
    "\n",
    "# Process each CSV file to extract price data\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file, skiprows=1, delimiter=r'\\s{2,}', engine='python')\n",
    "\n",
    "        # Clean and extract the 'Price' column\n",
    "        data_rows = [line.strip() for line in open(file, 'r').readlines()[1:] if line.strip() and ',' in line]\n",
    "        df_cleaned = pd.DataFrame([row.split(',') for row in data_rows], columns=['Date', 'Week', 'Week Day', 'Hour', 'Price', 'Volume', 'Sale/Purchase'])\n",
    "\n",
    "        # Convert 'Price' column to numeric\n",
    "        df_cleaned['Price'] = pd.to_numeric(df_cleaned['Price'], errors='coerce')\n",
    "\n",
    "        # Append the prices to the list\n",
    "        all_prices.extend(df_cleaned['Price'].dropna().tolist())\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Convert the list of all prices to a DataFrame for plotting\n",
    "all_prices_df = pd.DataFrame(all_prices, columns=['Price'])\n",
    "\n",
    "# Setting up the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Creating a distribution plot for the aggregated Price data\n",
    "sns.kdeplot(all_prices_df['Price'], bw_adjust=0.5)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Combined Price Distribution Curve')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "# Observation The highest density of prices is concentrated around the more moderate price levels."
   ],
   "id": "e08c5e05ae900561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Combine all the CSV files again to ensure we have the 'Date' column for proper monthly aggregation\n",
    "all_data = []\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, skiprows=1, delimiter=r'\\s{2,}', engine='python')\n",
    "\n",
    "        data_rows = [line.strip() for line in open(file, 'r').readlines()[1:] if line.strip() and ',' in line]\n",
    "        df_cleaned = pd.DataFrame([row.split(',') for row in data_rows], columns=['Date', 'Week', 'Week Day', 'Hour', 'Price', 'Volume', 'Sale/Purchase'])\n",
    "\n",
    "        df_cleaned['Price'] = pd.to_numeric(df_cleaned['Price'], errors='coerce')\n",
    "        df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'], errors='coerce')\n",
    "\n",
    "        all_data.append(df_cleaned[['Date', 'Price']])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Drop any rows with missing dates or prices\n",
    "combined_df.dropna(subset=['Date', 'Price'], inplace=True)\n",
    "\n",
    "# Extract year and month from the Date column\n",
    "combined_df['YearMonth'] = combined_df['Date'].dt.to_period('M')\n",
    "\n",
    "# Calculate the average price for each month\n",
    "monthly_avg_prices = combined_df.groupby('YearMonth')['Price'].mean().reset_index()\n",
    "\n",
    "# Convert the 'YearMonth' to a datetime object for proper plotting\n",
    "monthly_avg_prices['YearMonth'] = monthly_avg_prices['YearMonth'].dt.to_timestamp()\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Creating a line plot for the monthly average prices\n",
    "plt.plot(monthly_avg_prices['YearMonth'], monthly_avg_prices['Price'], marker='o')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Monthly Average Price Trends')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Price')\n",
    "plt.grid(True)\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n",
    "\n",
    "# Observation \n",
    "# There are noticeable fluctuations in average prices from month to month \n",
    "# Some months show a significant rise or drop in prices  \n",
    "# Certain months exhibit higher average prices compared to others"
   ],
   "id": "1989e5b29f74afb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Calculate the overall average price\n",
    "overall_avg_price = monthly_avg_prices['Price'].mean()\n",
    "\n",
    "# Identify months with average prices significantly higher than the overall average\n",
    "higher_than_avg = monthly_avg_prices[monthly_avg_prices['Price'] > overall_avg_price]\n",
    "\n",
    "# Display the months with higher than average prices\n",
    "higher_than_avg\n",
    "\n",
    "#Observation \n",
    "# The months with average prices significantly higher than the overall average are:\n",
    "\n",
    "#March 2022: Average Price = 171.04\n",
    "#April 2022: Average Price = 166.81\n",
    "#May 2022: Average Price = 166.85\n"
   ],
   "id": "47597813c8751781",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
